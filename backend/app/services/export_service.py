
import json
from sqlmodel import Session
from app.models.dataset import Dataset
from app.services import eda_service, insight_service

def generate_research_report(dataset_id: int, session: Session):
    """
    Functionality: Final Research Report Generation.
    Aggregates all analysis into a structured, academic-style report.
    """
    # 1. Fetch Core Data
    dataset = session.get(Dataset, dataset_id)
    if not dataset:
        return None
        
    insights = insight_service.generate_insights(dataset_id, session)
    ingestion_meta = json.loads(dataset.ingestion_insights) if dataset.ingestion_insights else {}
    processing_log = json.loads(dataset.processing_log) if dataset.processing_log else []
    
    # 2. Section 1: Dataset Identity
    identity = {
        "dataset_name": dataset.filename,
        "rows": dataset.row_count,
        "columns": dataset.column_count,
        "file_type": dataset.file_type.upper(),
        "status": "Prepared & Cleaned" if dataset.analyzed else "Raw Upload"
    }

    # 3. Section 2: Methodology (Transparent Process)
    methodology = [
        "Ingested and scanned the dataset for structural integrity.",
        f"Identified and removed {dataset.unstructured_row_removal_count} completely empty rows.",
        "Calculated descriptive statistics for all numeric columns.",
        "Performed frequency analysis on categorical variables.",
        "Assessed data quality and readiness for machine learning."
    ]
    if len(processing_log) > 0:
        methodology.append(f"Applied {len(processing_log)} automated cleaning operations.")
    methodology.append("Did NOT perform predictive modeling or value imputation (to preserve integrity).")

    # 4. Section 3: Key Findings (Interpretations)
    key_findings = []
    # Add patterns as text
    for p in insights.get("patterns", []):
         key_findings.append(f"{p['title']}: {p['explanation']}")
    
    # Add critical issue summaries
    high_issues = insights.get("issues", {}).get("high", [])
    if high_issues:
        key_findings.append(f"Critical Data Gaps: Found {len(high_issues)} high-severity issues affecting data reliability.")
    else:
        key_findings.append("Reliability: No critical data quality issues were found.")

    # 5. Section 4: Data Readiness
    readiness = {
        "status": ingestion_meta.get("readiness", {}).get("status", "Unknown"),
        "statement": ingestion_meta.get("readiness", {}).get("explanation", "Assessment pending."),
        "suitable_for": insights.get("good_for", []),
        "unsuitable_for": insights.get("not_good_for", [])
    }

    # 6. Section 5: Limitations (Honesty)
    limitations = insights.get("system_limits", [])
    # Add specific data limitations
    if dataset.quality_score < 80:
        limitations.append("Low quality score indicates potential bias or missing information.")
    if dataset.row_count < 50:
         limitations.append("Small sample size limits statistical significance.")

    # 7. Section 6: Recommendations
    recommendations = []
    all_issues = []
    for level in ["high", "medium", "info"]:
        all_issues.extend(insights.get("issues", {}).get(level, []))
    
    # Extract unique recommendations
    rec_set = set()
    for issue in all_issues:
        if "recommendation" in issue:
             rec_set.add(issue["recommendation"])
    
    # Add general recommendations if empty
    if not rec_set:
        rec_set.add("Proceed with visualization and descriptive reporting.")
        if dataset.quality_score < 100:
             rec_set.add("Review missing values in the Preparation tab.")
    
    recommendations = list(rec_set)

    # 8. Generation of Export Text (Markdown)
    markdown_report = f"""# Research Report: {identity['dataset_name']}
Date: {dataset.created_at}
Source: A.V.I.S. Analytics Platform

## 1. Dataset Identity
- **Name**: {identity['dataset_name']}
- **Dimensions**: {identity['rows']} rows x {identity['columns']} columns
- **Type**: {identity['file_type']}
- **Status**: {identity['status']}

## 2. Methodology
The following steps were taken to analyze this dataset:
{chr(10).join([f'- {m}' for m in methodology])}

## 3. Key Findings
{chr(10).join([f'- {k}' for k in key_findings])}

## 4. Data Readiness
**Status**: {readiness['status']}
**Assessment**: {readiness['statement']}

**Suitable For**:
{chr(10).join([f'- {s}' for s in readiness['suitable_for']])}

**NOT Suitable For**:
{chr(10).join([f'- {s}' for s in readiness['unsuitable_for']])}

## 5. Limitations
{chr(10).join([f'- {l}' for l in limitations])}

## 6. Recommendations
{chr(10).join([f'- {r}' for r in recommendations])}

---
*Generated by A.V.I.S. (Analytical Visual Intelligence System)*
"""

    return {
        "identity": identity,
        "methodology": methodology,
        "key_findings": key_findings,
        "readiness": readiness,
        "limitations": limitations,
        "recommendations": recommendations,
        "markdown_content": markdown_report
    }
