# backend/app/services/chat_service.py
import os
import json
from openai import OpenAI  # Switched to OpenAI SDK for Groq compatibility
from sqlmodel import Session
from app.services import eda_service
from app.models.dataset import Dataset
from app.core.config import settings

def get_client():
    """Initializes the Groq client using the OpenAI-compatible handshake."""
    return OpenAI(
        api_key=os.getenv("GROQ_API_KEY"),
        base_url="https://api.groq.com/openai/v1"
    )

def process_message(dataset_id: int, message: str, session: Session, page_context: dict = None):
    """
    Functionality 5: Context-Aware AI Assistance.
    Processes natural language queries using high-performance Groq models with page-specific context.
    """
    # 1. Retrieve the dataset record
    dataset = session.get(Dataset, dataset_id)
    if not dataset:
        return {"response": "Relational link failed: Dataset node not found."}

    # Fetch Contexts
    summary = eda_service.get_summary_statistics(dataset_id, session)
    
    # 2. Build the Intelligence Node prompt with explicit data grounding
    # We prioritize the PAGE CONTEXT if provided, as it represents what the user is currently looking at.
    
    context_str = "User is currently on the Dashboard Overview."
    if page_context:
        page_name = page_context.get("page", "Unknown Page")
        context_str = f"User is currently viewing the '{page_name.upper()}' Page.\n"
        context_str += f"VISIBLE DATA ON SCREEN: {json.dumps(page_context, indent=2)}"

    system_instruction = f"""
    You are A.V.I.S (Universal Data Guide), a friendly data teacher.
    Your student knows NOTHING about math or computers.
    
    CURRENT CONTEXT (What the user sees RIGHT NOW):
    {context_str}
    
    GLOBAL DATASET STATS:
    - Rows: {dataset.row_count}
    - Columns: {dataset.column_count}
    - Numeric Highlights: {json.dumps(summary.get('numeric'))}
    
    STRICT RULES:
    1. ROLE: Explain ONLY what the system found. Do NOT invent new analysis.
    2. SCOPE: Focus your answer on the '{page_context.get('page') if page_context else 'General'}' context.
    3. TONE: Beginner-friendly. No jargon. Use analogies.
    4. SAFETY: If the user asks for a prediction, say "I cannot predict the future, but I can explain the past data."
    5. VISUALS: If a chart would help, suggest one using [PLOT_CONFIG].
    
    FORBIDDEN WORDS:
    - "I assume"
    - "Likely"
    - "Maybe"
    - "Train a model"
    
    PLOT FORMAT (Only use if needed):
    [PLOT_CONFIG: {{"chartType": "bar|scatter|line|pie", "xColumn": "col1", "yColumn": "col2"}}]
    """

    try:
        # 3. Generate response using Groq's high-speed Llama model
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": message}
            ],
            temperature=0.7
        )
        
        text = response.choices[0].message.content
        plot_config = None

        # 4. Extract Plotting Instructions if generated by the AI
        if "[PLOT_CONFIG:" in text:
            try:
                # Parse the JSON config from the AI response
                config_str = text.split("[PLOT_CONFIG:")[1].split("]")[0]
                plot_config = json.loads(config_str)
                # Clean the visible text response to hide raw JSON from user
                text = text.split("[PLOT_CONFIG:")[0].strip()
            except Exception:
                pass

        return {
            "response": text,
            "plot_config": plot_config
        }
        
    except Exception as e:
        # Advanced Error Handling: Detect quota issues without crashing
        error_msg = str(e)
        if "429" in error_msg:
            return {"response": "The Intelligence Node is resting (Quota limit reached). Please try again later."}
        return {"response": f"Intelligence Node Anomaly: {error_msg}"}